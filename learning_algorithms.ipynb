{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to use reinforcement learning to learn a policy of routes for a trash collector truck. An optimized oath can help in obtaining a faster and more efficient trash collection. The problem can be modeled as identifying a path inside a graph. This problem have been previously solved using other optimization approachs, heuristics and graph algorithms, however we want to apply reinforcement learning in it for educational purposes and compare it with other approaches. The problem can be defined as follow:\n",
    "\n",
    "**Def. 1**: With a weighted directed graph $G = (V, E)$, and a starting node $v$, find the shortest path to node $u$.\n",
    "\n",
    "**Def. 2**: With a weighted directed graph $G = (V, E)$, and a set of nodes $U = \\{u_1, \\dots, u_k\\}$, find the shortest path that traverse all nodes.\n",
    "\n",
    "The second definition is a harder problem in which the model has to learn the best order of collection. The problem can be modeled in the reinforcement learning format as follow:\n",
    "\n",
    "**Reinforcement learning model**:\n",
    "- Episodic, finishes when arrive on destination node.\n",
    "- States: $n$ states $v_i \\in V$ , each is a node of the graph.\n",
    "- Actions: $n$ action, each one correspond to moving to a specific node. It is important to note that the action to move to node $u$, if the state is node $v$, can only be made if there is an edge from $v$ to $u$.\n",
    "- Reward will be $0$ at any node and will be $1$ if the node is the desired one (or one of the set of desired nodes). Impossible actions (moving trought a non-existent edge) will have reward equal to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial\n",
    "\n",
    "https://www.deeplearningbook.com.br/algoritmo-de-agente-baseado-em-ia-com-reinforcement-learning-q-learning/\n",
    "\n",
    "\n",
    "$$Q^{\\pi} (s_t, a_t) = \\mathbb{E} \\left[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+ \\dots | s_t, a_t\\right]$$\n",
    "\n",
    "$$Q (s_t, a_t) \\leftarrow Q (s_t, a_t) + \\delta \\left[ R_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Deterministic simulation that returns the state based in the action.\n",
    "    It also returns the reward in the following format:\n",
    "        -1: if the action is not valid\n",
    "        0: if the action is valid but the destination is not reached\n",
    "        1: if the action is valid and the destination is reached\n",
    "    \"\"\"\n",
    "    def __init__(self, G, source, dest, reward = \"unit\"):\n",
    "        self.G = G\n",
    "        self.source = source\n",
    "        self.dest = dest\n",
    "        self.reward = reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.source\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Return new state, reward, and if the destination is reached\"\"\"\n",
    "        # action is a node, verify if it is a neighbor\n",
    "        # if is not a neighbor, stay in the same state\n",
    "        if self.reward == \"unit\":\n",
    "            neighbors = list(self.G.neighbors(self.state))\n",
    "            if action not in neighbors:\n",
    "                return self.state, -1, False\n",
    "            \n",
    "            self.state = action\n",
    "            if self.state == self.dest:\n",
    "                return self.state, 1, True\n",
    "            else:\n",
    "                return self.state, 0, False\n",
    "        elif self.reward == \"weighted\":\n",
    "            neighbors = list(self.G.neighbors(self.state))\n",
    "            if action not in neighbors:\n",
    "                return self.state, -100, False\n",
    "            \n",
    "            if action == self.dest:\n",
    "                self.state = action\n",
    "                return self.state, 100, True\n",
    "            else:\n",
    "                w = self.G[self.state][action]['weight']\n",
    "                self.state = action\n",
    "                return self.state, -w, False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q, state):\n",
    "    \"\"\"Greedy policy that returns the action with the highest Q value\"\"\"\n",
    "    return np.argmax(Q[state, :])\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    \"\"\"Epsilon greedy policy that returns a random action with probability epsilon\"\"\"\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(len(Q[state, :]))\n",
    "    else:\n",
    "        return greedy_policy(Q, state)\n",
    "\n",
    "def train_q_learning(\n",
    "    Q,\n",
    "    learning_rate = 0.7,\n",
    "    gamma = 0.95,\n",
    "    min_epsilon = 0.05,\n",
    "    max_epsilon = 1,\n",
    "    n_episodes = 10000,\n",
    "    max_steps = 100,\n",
    "):\n",
    "    \"\"\"Training of the Q table using Q learning algorithm. \n",
    "    It needs the enviroment env and the Q table.\n",
    "\n",
    "    :param Q: numpy array with the Q table\n",
    "    :param learning_rate: learning rate of algorithm, defaults to 0.7\n",
    "    :param gamma: weight of future rewards, defaults to 0.95\n",
    "    :param min_epsilon: min probability of exploration, defaults to 0.05\n",
    "    :param max_epsilon: max probability of exploration, defaults to 1\n",
    "    :param n_episodes: number of episodes, defaults to 10000\n",
    "    :param max_steps: max number of steps per episode, defaults to 100\n",
    "    \"\"\"\n",
    "    epsilon = max_epsilon\n",
    "    rechead_dest = False\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        # Start episode again resetting the enviroment\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Choose action and get reward\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            new_state, reward, done = env.step(action)\n",
    "            \n",
    "            # update Q table based on Bellman equation\n",
    "            Q[state, action] += learning_rate * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
    "            state = new_state\n",
    "            if done and not rechead_dest:\n",
    "                rechead_dest = True\n",
    "                print(f\"Episode {episode} reached destination in {step} steps\")\n",
    "        \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        epsilon -= (max_epsilon - min_epsilon) / n_episodes # make it decay exponentially\n",
    "\n",
    "\n",
    "def test_q_learning(Q):\n",
    "    state = env.reset()\n",
    "    actions = [state]\n",
    "    for _ in range(1000):\n",
    "        action = greedy_policy(Q, state)\n",
    "        state, _, done = env.step(action)\n",
    "        actions.append(action)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path between first and last node: [0, 6, 320, 999]\n",
      "Weight of the shortest path: 11\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 1000\n",
    "G = nx.barabasi_albert_graph(n_nodes, 3)\n",
    "# if there is an edge between 0 and 99, remove it\n",
    "if G.has_edge(0, n_nodes - 1):\n",
    "    G.remove_edge(0, n_nodes - 1)\n",
    "# add random weights to edges\n",
    "for edge in G.edges():\n",
    "    G.edges[edge][\"weight\"] = np.random.randint(1, 10)\n",
    "print(\"Path between first and last node:\", nx.shortest_path(G, 0, n_nodes - 1))\n",
    "# print weight of the shortest path\n",
    "print(\"Weight of the shortest path:\", nx.shortest_path_length(G, 0, n_nodes - 1, weight = \"weight\"))\n",
    "\n",
    "\n",
    "\n",
    "if n_nodes < 200:\n",
    "    nx.draw(G)\n",
    "env = Environment(G, 0, n_nodes - 1, reward = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 1637/10000 [00:01<00:05, 1404.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1334 reached destination in 84 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4080.88it/s]\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((len(G.nodes), len(G.nodes)))\n",
    "train_q_learning(Q, min_epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 413, 406, 999]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_q_learning(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of the path: 16\n"
     ]
    }
   ],
   "source": [
    "path = [0, 413, 406, 999]\n",
    "lenght = 0\n",
    "for i in range(len(path) - 1):\n",
    "    lenght += G[path[i]][path[i + 1]]['weight']\n",
    "print(\"Lenght of the path:\", lenght)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
