{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "from plotting import *\n",
    "from enviroment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    960703545, 1277478588, 1936856304, 186872697, 1859168769, 1598189534, 1822174485, 1871883252, 694388766,\n",
    "    188312339, 773370613, 2125204119, 2041095833, 1384311643, 1000004583, 358485174, 1695858027, 762772169,\n",
    "    437720306, 939612284\n",
    "]\n",
    "G = ox.graph_from_address('Campinas, São Paulo', network_type='drive')\n",
    "G = nx.convert_node_labels_to_integers(G)\n",
    "source = 507\n",
    "target = 235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, values):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(values)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, states_dim, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(states_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, learning_rate = 0.3, gamma = 0.99, tau = 0.05, max_epsilon = 1, min_epsilon = 0.1, n_episodes = 1000, max_steps = 1000, batch_size = 64):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.n_episodes = n_episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.n_states = env.get_n_states()\n",
    "        self.policy_net = DQN(self.n_states + 2, self.n_states)\n",
    "        self.target_net = DQN(self.n_states + 2, self.n_states)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        #self.policy_net.to(device)\n",
    "        #self.target_net.to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.memory = ReplayMemory(1000)\n",
    "\n",
    "    def get_features(self, state):\n",
    "        \"\"\"Return a torch tensor with the features of the state\"\"\"\n",
    "        features = torch.zeros(self.n_states + 2, dtype=torch.float32)#, device=device)\n",
    "        features[state] = 1\n",
    "        features[self.n_states] = self.env.G.nodes[state][\"x\"]\n",
    "        features[self.n_states + 1] = self.env.G.nodes[state][\"y\"]\n",
    "        return features\n",
    "    \n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon -= (self.max_epsilon - self.min_epsilon) / self.n_episodes\n",
    "        \n",
    "\n",
    "    def greedy_policy(self, state):\n",
    "        \"\"\"Greedy policy that returns the action with the highest Q value\"\"\"\n",
    "        neighbors = list(self.env.G.neighbors(state))\n",
    "        # transform neighbors to boolean array\n",
    "        neighbors = [True if neighbor in neighbors else False for neighbor in range(self.n_states)]\n",
    "        actions_values = self.policy_net(self.get_features(state))\n",
    "        # make non-neighbors equal to -inf so they are not chosen\n",
    "        neighbors_boolean = torch.tensor(neighbors, dtype=torch.bool) #, device=device)\n",
    "        actions_values[~neighbors_boolean] = -float(\"Inf\")\n",
    "        return actions_values.argmax().view(1).item()\n",
    "    \n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon):\n",
    "        \"\"\"Epsilon greedy policy that returns a random action with probability epsilon\"\"\"\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            neighbors = list(self.env.G.neighbors(state))\n",
    "            return random.choice(neighbors)\n",
    "        else:\n",
    "            return self.greedy_policy(state)\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        self.epsilon = self.max_epsilon\n",
    "        self.episode_rewards = []\n",
    "        for episode in tqdm(range(self.n_episodes)):\n",
    "            self.generate_episode(self.epsilon)\n",
    "            self.update_epsilon()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.get_policy()\n",
    "\n",
    "    \n",
    "    def generate_episode(self, epsilon):\n",
    "        state = self.env.reset()\n",
    "        state_features = self.get_features(state)\n",
    "        self.episode_rewards.append(0)\n",
    "\n",
    "        for step in range(self.max_steps):\n",
    "            # Choose action and get reward\n",
    "            action = self.epsilon_greedy_policy(state, epsilon)\n",
    "            new_state, reward, done = self.env.step(action)\n",
    "            self.episode_rewards[-1] += reward\n",
    "            new_state_features = self.get_features(new_state)\n",
    "\n",
    "            self.memory.push(torch.cat([state_features, new_state_features, torch.tensor([action, reward], dtype=torch.float32)]))\n",
    "            state = new_state\n",
    "\n",
    "            self.optimize_model()\n",
    "\n",
    "            # soft update of weights\n",
    "            target_net_state_dict = self.target_net.state_dict()\n",
    "            policy_net_state_dict = self.policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "            self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = torch.stack(self.memory.sample(self.batch_size))\n",
    "        \n",
    "        state_batch = transitions[:, :self.n_states + 2].view(-1, self.n_states + 2)\n",
    "        new_state_batch = transitions[:, (self.n_states+2):2*(self.n_states+2)].view(-1, self.n_states + 2)\n",
    "        action_batch = transitions[:, -2].view(-1, 1).long()\n",
    "        reward_batch = transitions[:, -1].view(-1, 1)\n",
    "        \n",
    "        Q_s_a = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        Q_s_a_prime = self.target_net(new_state_batch).max(dim=1)[0].view(-1, 1).detach()\n",
    "        \n",
    "        \n",
    "        loss = F.mse_loss(Q_s_a, reward_batch + self.gamma * Q_s_a_prime)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def get_policy(self):\n",
    "        states_features = [self.get_features(state) for state in range(self.env.get_n_states())]\n",
    "        states_features = torch.stack(states_features)\n",
    "        states_values = self.policy_net(states_features).cpu().numpy()\n",
    "        self.policy = states_values.argmax(axis=1)\n",
    "        self.policy = {state: action for state, action in enumerate(self.policy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(G, source, target, \"weighted\")\n",
    "agent = DQNAgent(env, n_episodes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:14<14:20,  9.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [147]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes)):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_epsilon()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36mDQNAgent.generate_episode\u001b[0;34m(self, epsilon)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(torch\u001b[38;5;241m.\u001b[39mcat([state_features, new_state_features, torch\u001b[38;5;241m.\u001b[39mtensor([action, reward], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)]))\n\u001b[1;32m    106\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# soft update of weights\u001b[39;00m\n\u001b[1;32m    111\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36mDQNAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m transitions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    130\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m transitions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m Q_s_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m    133\u001b[0m Q_s_a_prime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net(new_state_batch)\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    136\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(Q_s_a, reward_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m Q_s_a_prime)\n",
      "File \u001b[0;32m~/anaconda3/envs/ox/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ox/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [136]\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ox/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ox/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ox/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000, -47.0632, -22.9120])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_features(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.greedy_policy(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epsilon_greedy_policy(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.2910e+01,\n",
       "         5.7200e+02, -9.2886e-02])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.sample(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1216])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.sample(5)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.sample(5)[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 592,\n",
       " 1: 592,\n",
       " 2: 592,\n",
       " 3: 592,\n",
       " 4: 592,\n",
       " 5: 592,\n",
       " 6: 592,\n",
       " 7: 592,\n",
       " 8: 592,\n",
       " 9: 592,\n",
       " 10: 592,\n",
       " 11: 592,\n",
       " 12: 592,\n",
       " 13: 592,\n",
       " 14: 592,\n",
       " 15: 592,\n",
       " 16: 592,\n",
       " 17: 592,\n",
       " 18: 592,\n",
       " 19: 592,\n",
       " 20: 592,\n",
       " 21: 592,\n",
       " 22: 592,\n",
       " 23: 592,\n",
       " 24: 592,\n",
       " 25: 592,\n",
       " 26: 592,\n",
       " 27: 592,\n",
       " 28: 592,\n",
       " 29: 592,\n",
       " 30: 592,\n",
       " 31: 592,\n",
       " 32: 592,\n",
       " 33: 592,\n",
       " 34: 592,\n",
       " 35: 592,\n",
       " 36: 592,\n",
       " 37: 592,\n",
       " 38: 592,\n",
       " 39: 592,\n",
       " 40: 592,\n",
       " 41: 592,\n",
       " 42: 592,\n",
       " 43: 592,\n",
       " 44: 592,\n",
       " 45: 592,\n",
       " 46: 592,\n",
       " 47: 592,\n",
       " 48: 592,\n",
       " 49: 592,\n",
       " 50: 592,\n",
       " 51: 592,\n",
       " 52: 592,\n",
       " 53: 592,\n",
       " 54: 592,\n",
       " 55: 592,\n",
       " 56: 592,\n",
       " 57: 592,\n",
       " 58: 592,\n",
       " 59: 592,\n",
       " 60: 592,\n",
       " 61: 592,\n",
       " 62: 592,\n",
       " 63: 592,\n",
       " 64: 592,\n",
       " 65: 592,\n",
       " 66: 592,\n",
       " 67: 592,\n",
       " 68: 592,\n",
       " 69: 592,\n",
       " 70: 592,\n",
       " 71: 592,\n",
       " 72: 592,\n",
       " 73: 592,\n",
       " 74: 592,\n",
       " 75: 592,\n",
       " 76: 592,\n",
       " 77: 592,\n",
       " 78: 592,\n",
       " 79: 592,\n",
       " 80: 592,\n",
       " 81: 592,\n",
       " 82: 592,\n",
       " 83: 592,\n",
       " 84: 592,\n",
       " 85: 592,\n",
       " 86: 592,\n",
       " 87: 592,\n",
       " 88: 592,\n",
       " 89: 592,\n",
       " 90: 592,\n",
       " 91: 592,\n",
       " 92: 592,\n",
       " 93: 592,\n",
       " 94: 592,\n",
       " 95: 592,\n",
       " 96: 592,\n",
       " 97: 592,\n",
       " 98: 592,\n",
       " 99: 592,\n",
       " 100: 592,\n",
       " 101: 592,\n",
       " 102: 592,\n",
       " 103: 592,\n",
       " 104: 592,\n",
       " 105: 592,\n",
       " 106: 592,\n",
       " 107: 592,\n",
       " 108: 592,\n",
       " 109: 592,\n",
       " 110: 592,\n",
       " 111: 592,\n",
       " 112: 592,\n",
       " 113: 592,\n",
       " 114: 592,\n",
       " 115: 592,\n",
       " 116: 592,\n",
       " 117: 592,\n",
       " 118: 592,\n",
       " 119: 592,\n",
       " 120: 592,\n",
       " 121: 592,\n",
       " 122: 592,\n",
       " 123: 592,\n",
       " 124: 592,\n",
       " 125: 592,\n",
       " 126: 592,\n",
       " 127: 592,\n",
       " 128: 592,\n",
       " 129: 592,\n",
       " 130: 592,\n",
       " 131: 592,\n",
       " 132: 592,\n",
       " 133: 592,\n",
       " 134: 592,\n",
       " 135: 592,\n",
       " 136: 592,\n",
       " 137: 592,\n",
       " 138: 592,\n",
       " 139: 592,\n",
       " 140: 592,\n",
       " 141: 592,\n",
       " 142: 592,\n",
       " 143: 592,\n",
       " 144: 592,\n",
       " 145: 592,\n",
       " 146: 592,\n",
       " 147: 592,\n",
       " 148: 592,\n",
       " 149: 592,\n",
       " 150: 592,\n",
       " 151: 592,\n",
       " 152: 592,\n",
       " 153: 592,\n",
       " 154: 592,\n",
       " 155: 592,\n",
       " 156: 592,\n",
       " 157: 592,\n",
       " 158: 592,\n",
       " 159: 592,\n",
       " 160: 592,\n",
       " 161: 592,\n",
       " 162: 592,\n",
       " 163: 592,\n",
       " 164: 592,\n",
       " 165: 592,\n",
       " 166: 592,\n",
       " 167: 592,\n",
       " 168: 592,\n",
       " 169: 592,\n",
       " 170: 592,\n",
       " 171: 592,\n",
       " 172: 592,\n",
       " 173: 592,\n",
       " 174: 592,\n",
       " 175: 592,\n",
       " 176: 592,\n",
       " 177: 592,\n",
       " 178: 592,\n",
       " 179: 592,\n",
       " 180: 592,\n",
       " 181: 592,\n",
       " 182: 592,\n",
       " 183: 592,\n",
       " 184: 592,\n",
       " 185: 592,\n",
       " 186: 592,\n",
       " 187: 592,\n",
       " 188: 592,\n",
       " 189: 592,\n",
       " 190: 592,\n",
       " 191: 592,\n",
       " 192: 592,\n",
       " 193: 592,\n",
       " 194: 592,\n",
       " 195: 592,\n",
       " 196: 592,\n",
       " 197: 592,\n",
       " 198: 592,\n",
       " 199: 592,\n",
       " 200: 592,\n",
       " 201: 592,\n",
       " 202: 592,\n",
       " 203: 592,\n",
       " 204: 592,\n",
       " 205: 592,\n",
       " 206: 592,\n",
       " 207: 592,\n",
       " 208: 592,\n",
       " 209: 592,\n",
       " 210: 592,\n",
       " 211: 592,\n",
       " 212: 592,\n",
       " 213: 592,\n",
       " 214: 592,\n",
       " 215: 592,\n",
       " 216: 592,\n",
       " 217: 592,\n",
       " 218: 592,\n",
       " 219: 592,\n",
       " 220: 592,\n",
       " 221: 592,\n",
       " 222: 592,\n",
       " 223: 592,\n",
       " 224: 592,\n",
       " 225: 592,\n",
       " 226: 592,\n",
       " 227: 592,\n",
       " 228: 592,\n",
       " 229: 592,\n",
       " 230: 592,\n",
       " 231: 592,\n",
       " 232: 592,\n",
       " 233: 592,\n",
       " 234: 592,\n",
       " 235: 592,\n",
       " 236: 592,\n",
       " 237: 592,\n",
       " 238: 592,\n",
       " 239: 592,\n",
       " 240: 592,\n",
       " 241: 592,\n",
       " 242: 592,\n",
       " 243: 592,\n",
       " 244: 592,\n",
       " 245: 592,\n",
       " 246: 592,\n",
       " 247: 592,\n",
       " 248: 592,\n",
       " 249: 592,\n",
       " 250: 592,\n",
       " 251: 592,\n",
       " 252: 592,\n",
       " 253: 592,\n",
       " 254: 592,\n",
       " 255: 592,\n",
       " 256: 592,\n",
       " 257: 592,\n",
       " 258: 592,\n",
       " 259: 592,\n",
       " 260: 592,\n",
       " 261: 592,\n",
       " 262: 592,\n",
       " 263: 592,\n",
       " 264: 592,\n",
       " 265: 592,\n",
       " 266: 592,\n",
       " 267: 592,\n",
       " 268: 592,\n",
       " 269: 592,\n",
       " 270: 592,\n",
       " 271: 592,\n",
       " 272: 592,\n",
       " 273: 592,\n",
       " 274: 592,\n",
       " 275: 592,\n",
       " 276: 592,\n",
       " 277: 592,\n",
       " 278: 592,\n",
       " 279: 592,\n",
       " 280: 592,\n",
       " 281: 592,\n",
       " 282: 592,\n",
       " 283: 592,\n",
       " 284: 592,\n",
       " 285: 592,\n",
       " 286: 592,\n",
       " 287: 592,\n",
       " 288: 592,\n",
       " 289: 592,\n",
       " 290: 592,\n",
       " 291: 592,\n",
       " 292: 592,\n",
       " 293: 592,\n",
       " 294: 592,\n",
       " 295: 592,\n",
       " 296: 592,\n",
       " 297: 592,\n",
       " 298: 592,\n",
       " 299: 592,\n",
       " 300: 592,\n",
       " 301: 592,\n",
       " 302: 592,\n",
       " 303: 592,\n",
       " 304: 592,\n",
       " 305: 592,\n",
       " 306: 592,\n",
       " 307: 592,\n",
       " 308: 592,\n",
       " 309: 592,\n",
       " 310: 592,\n",
       " 311: 592,\n",
       " 312: 592,\n",
       " 313: 592,\n",
       " 314: 592,\n",
       " 315: 592,\n",
       " 316: 592,\n",
       " 317: 592,\n",
       " 318: 592,\n",
       " 319: 592,\n",
       " 320: 592,\n",
       " 321: 592,\n",
       " 322: 592,\n",
       " 323: 592,\n",
       " 324: 592,\n",
       " 325: 592,\n",
       " 326: 592,\n",
       " 327: 592,\n",
       " 328: 592,\n",
       " 329: 592,\n",
       " 330: 592,\n",
       " 331: 592,\n",
       " 332: 592,\n",
       " 333: 592,\n",
       " 334: 592,\n",
       " 335: 592,\n",
       " 336: 592,\n",
       " 337: 592,\n",
       " 338: 592,\n",
       " 339: 592,\n",
       " 340: 592,\n",
       " 341: 592,\n",
       " 342: 592,\n",
       " 343: 592,\n",
       " 344: 592,\n",
       " 345: 592,\n",
       " 346: 592,\n",
       " 347: 592,\n",
       " 348: 592,\n",
       " 349: 592,\n",
       " 350: 592,\n",
       " 351: 592,\n",
       " 352: 592,\n",
       " 353: 592,\n",
       " 354: 592,\n",
       " 355: 592,\n",
       " 356: 592,\n",
       " 357: 592,\n",
       " 358: 592,\n",
       " 359: 592,\n",
       " 360: 592,\n",
       " 361: 592,\n",
       " 362: 592,\n",
       " 363: 592,\n",
       " 364: 592,\n",
       " 365: 592,\n",
       " 366: 592,\n",
       " 367: 592,\n",
       " 368: 592,\n",
       " 369: 592,\n",
       " 370: 592,\n",
       " 371: 592,\n",
       " 372: 592,\n",
       " 373: 592,\n",
       " 374: 592,\n",
       " 375: 592,\n",
       " 376: 592,\n",
       " 377: 592,\n",
       " 378: 592,\n",
       " 379: 592,\n",
       " 380: 592,\n",
       " 381: 592,\n",
       " 382: 592,\n",
       " 383: 592,\n",
       " 384: 592,\n",
       " 385: 592,\n",
       " 386: 592,\n",
       " 387: 592,\n",
       " 388: 592,\n",
       " 389: 592,\n",
       " 390: 592,\n",
       " 391: 592,\n",
       " 392: 592,\n",
       " 393: 592,\n",
       " 394: 592,\n",
       " 395: 592,\n",
       " 396: 592,\n",
       " 397: 592,\n",
       " 398: 592,\n",
       " 399: 592,\n",
       " 400: 592,\n",
       " 401: 592,\n",
       " 402: 592,\n",
       " 403: 592,\n",
       " 404: 592,\n",
       " 405: 592,\n",
       " 406: 592,\n",
       " 407: 592,\n",
       " 408: 592,\n",
       " 409: 592,\n",
       " 410: 592,\n",
       " 411: 592,\n",
       " 412: 592,\n",
       " 413: 592,\n",
       " 414: 592,\n",
       " 415: 592,\n",
       " 416: 592,\n",
       " 417: 592,\n",
       " 418: 592,\n",
       " 419: 592,\n",
       " 420: 592,\n",
       " 421: 592,\n",
       " 422: 592,\n",
       " 423: 592,\n",
       " 424: 592,\n",
       " 425: 592,\n",
       " 426: 592,\n",
       " 427: 592,\n",
       " 428: 592,\n",
       " 429: 592,\n",
       " 430: 592,\n",
       " 431: 592,\n",
       " 432: 592,\n",
       " 433: 592,\n",
       " 434: 592,\n",
       " 435: 592,\n",
       " 436: 592,\n",
       " 437: 592,\n",
       " 438: 592,\n",
       " 439: 592,\n",
       " 440: 592,\n",
       " 441: 592,\n",
       " 442: 592,\n",
       " 443: 592,\n",
       " 444: 592,\n",
       " 445: 592,\n",
       " 446: 592,\n",
       " 447: 592,\n",
       " 448: 592,\n",
       " 449: 592,\n",
       " 450: 592,\n",
       " 451: 592,\n",
       " 452: 592,\n",
       " 453: 592,\n",
       " 454: 592,\n",
       " 455: 592,\n",
       " 456: 592,\n",
       " 457: 592,\n",
       " 458: 592,\n",
       " 459: 592,\n",
       " 460: 592,\n",
       " 461: 592,\n",
       " 462: 592,\n",
       " 463: 592,\n",
       " 464: 592,\n",
       " 465: 592,\n",
       " 466: 592,\n",
       " 467: 592,\n",
       " 468: 592,\n",
       " 469: 592,\n",
       " 470: 592,\n",
       " 471: 592,\n",
       " 472: 592,\n",
       " 473: 592,\n",
       " 474: 592,\n",
       " 475: 592,\n",
       " 476: 592,\n",
       " 477: 592,\n",
       " 478: 592,\n",
       " 479: 592,\n",
       " 480: 592,\n",
       " 481: 592,\n",
       " 482: 592,\n",
       " 483: 592,\n",
       " 484: 592,\n",
       " 485: 592,\n",
       " 486: 592,\n",
       " 487: 592,\n",
       " 488: 592,\n",
       " 489: 592,\n",
       " 490: 592,\n",
       " 491: 592,\n",
       " 492: 592,\n",
       " 493: 592,\n",
       " 494: 592,\n",
       " 495: 592,\n",
       " 496: 592,\n",
       " 497: 592,\n",
       " 498: 592,\n",
       " 499: 592,\n",
       " 500: 592,\n",
       " 501: 592,\n",
       " 502: 592,\n",
       " 503: 592,\n",
       " 504: 592,\n",
       " 505: 592,\n",
       " 506: 592,\n",
       " 507: 592,\n",
       " 508: 592,\n",
       " 509: 592,\n",
       " 510: 592,\n",
       " 511: 592,\n",
       " 512: 592,\n",
       " 513: 592,\n",
       " 514: 592,\n",
       " 515: 592,\n",
       " 516: 592,\n",
       " 517: 592,\n",
       " 518: 592,\n",
       " 519: 592,\n",
       " 520: 592,\n",
       " 521: 592,\n",
       " 522: 592,\n",
       " 523: 592,\n",
       " 524: 592,\n",
       " 525: 592,\n",
       " 526: 592,\n",
       " 527: 592,\n",
       " 528: 592,\n",
       " 529: 592,\n",
       " 530: 592,\n",
       " 531: 592,\n",
       " 532: 592,\n",
       " 533: 592,\n",
       " 534: 592,\n",
       " 535: 592,\n",
       " 536: 592,\n",
       " 537: 592,\n",
       " 538: 592,\n",
       " 539: 592,\n",
       " 540: 592,\n",
       " 541: 592,\n",
       " 542: 592,\n",
       " 543: 592,\n",
       " 544: 592,\n",
       " 545: 592,\n",
       " 546: 592,\n",
       " 547: 592,\n",
       " 548: 592,\n",
       " 549: 592,\n",
       " 550: 592,\n",
       " 551: 592,\n",
       " 552: 592,\n",
       " 553: 592,\n",
       " 554: 592,\n",
       " 555: 592,\n",
       " 556: 592,\n",
       " 557: 592,\n",
       " 558: 592,\n",
       " 559: 592,\n",
       " 560: 592,\n",
       " 561: 592,\n",
       " 562: 592,\n",
       " 563: 592,\n",
       " 564: 592,\n",
       " 565: 592,\n",
       " 566: 592,\n",
       " 567: 592,\n",
       " 568: 592,\n",
       " 569: 592,\n",
       " 570: 592,\n",
       " 571: 592,\n",
       " 572: 592,\n",
       " 573: 592,\n",
       " 574: 592,\n",
       " 575: 592,\n",
       " 576: 592,\n",
       " 577: 592,\n",
       " 578: 592,\n",
       " 579: 592,\n",
       " 580: 592,\n",
       " 581: 592,\n",
       " 582: 592,\n",
       " 583: 592,\n",
       " 584: 592,\n",
       " 585: 592,\n",
       " 586: 592,\n",
       " 587: 592,\n",
       " 588: 592,\n",
       " 589: 592,\n",
       " 590: 592,\n",
       " 591: 592,\n",
       " 592: 592,\n",
       " 593: 592,\n",
       " 594: 592,\n",
       " 595: 592,\n",
       " 596: 592,\n",
       " 597: 592,\n",
       " 598: 592,\n",
       " 599: 592,\n",
       " 600: 592,\n",
       " 601: 592,\n",
       " 602: 592,\n",
       " 603: 592,\n",
       " 604: 592}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
